{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa468457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ba4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with rows: 84\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_20220411_106246.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_20220411_106256.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_20220411_106258.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_20220411_09235385.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_20220411_09235389.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename label\n",
       "0    IMG_20220411_106246.jpg     1\n",
       "1    IMG_20220411_106256.jpg     0\n",
       "2    IMG_20220411_106258.jpg     0\n",
       "3  IMG_20220411_09235385.jpg     0\n",
       "4  IMG_20220411_09235389.jpg     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV file \n",
    "csv_path = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\cleaned_labels.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Loaded CSV with rows:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f160d121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched image files: 84 / 84\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_20220411_106246.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_20220411_106256.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_20220411_106258.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_20220411_09235385.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_20220411_09235389.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename label\n",
       "0    IMG_20220411_106246.jpg     1\n",
       "1    IMG_20220411_106256.jpg     0\n",
       "2    IMG_20220411_106258.jpg     0\n",
       "3  IMG_20220411_09235385.jpg     0\n",
       "4  IMG_20220411_09235389.jpg     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_dir = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\dataset\\content\\dataset\"\n",
    "\n",
    "\n",
    "df['exists'] = df['filename'].apply(lambda x: os.path.exists(os.path.join(image_dir, x)))\n",
    "\n",
    "\n",
    "print(\"Matched image files:\", df['exists'].sum(), \"/\", len(df))\n",
    "\n",
    "\n",
    "df = df[df['exists']].copy()\n",
    "df = df[['filename', 'label']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e973c498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final verified labels saved at: C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\final_verified_labels.csv\n"
     ]
    }
   ],
   "source": [
    "final_csv_path = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\final_verified_labels.csv\"\n",
    "df.to_csv(final_csv_path, index=False)\n",
    "print(\"Final verified labels saved at:\", final_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d632b079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TASK 1 COMPLETE: 84 image-label pairs are verified and ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IMG_20220413_09235637.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>IMG_20220429_09237611.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>IMG_20220511_09239333.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>IMG_20220620_09247425.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IMG_20220426_09237108.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename label\n",
       "10  IMG_20220413_09235637.jpg     0\n",
       "45  IMG_20220429_09237611.jpg     0\n",
       "50  IMG_20220511_09239333.jpg     0\n",
       "75  IMG_20220620_09247425.jpg     0\n",
       "38  IMG_20220426_09237108.jpg     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\" TASK 1 COMPLETE: {len(df)} image-label pairs are verified and ready.\")\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7dac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adda0d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 84\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_20220411_106246.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_20220411_106256.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_20220411_106258.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_20220411_09235385.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_20220411_09235389.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename label\n",
       "0    IMG_20220411_106246.jpg     1\n",
       "1    IMG_20220411_106256.jpg     0\n",
       "2    IMG_20220411_106258.jpg     0\n",
       "3  IMG_20220411_09235385.jpg     0\n",
       "4  IMG_20220411_09235389.jpg     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\final_verified_labels.csv\"\n",
    "image_dir = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\dataset\\content\\dataset\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bcb8873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    filename label\n",
      "0    IMG_20220411_106246.jpg     1\n",
      "1    IMG_20220411_106256.jpg     0\n",
      "2    IMG_20220411_106258.jpg     0\n",
      "3  IMG_20220411_09235385.jpg     0\n",
      "4  IMG_20220411_09235389.jpg     0\n",
      "['1' '0' 'result not found']\n",
      "label\n",
      "0                   66\n",
      "1                   16\n",
      "result not found     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\cleaned_labels.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.head())                    # View top rows\n",
    "print(df['label'].unique())         # Check unique label values\n",
    "print(df['label'].value_counts())   # Count how many of each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915a7bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 COVID-positive images to augment.\n",
      "Saved augmented image: IMG_20220411_106246_aug_0.jpg\n",
      "Saved augmented image: IMG_20220411_106246_aug_1.jpg\n",
      "Saved augmented image: IMG_20220411_106246_aug_2.jpg\n",
      "Saved augmented image: IMG_20220411_106246_aug_3.jpg\n",
      "Saved augmented image: IMG_20220411_106246_aug_4.jpg\n",
      "Saved augmented image: IMG_20220413_09235579_aug_0.jpg\n",
      "Saved augmented image: IMG_20220413_09235579_aug_1.jpg\n",
      "Saved augmented image: IMG_20220413_09235579_aug_2.jpg\n",
      "Saved augmented image: IMG_20220413_09235579_aug_3.jpg\n",
      "Saved augmented image: IMG_20220413_09235579_aug_4.jpg\n",
      "Saved augmented image: IMG_20220413_09235624_aug_0.jpg\n",
      "Saved augmented image: IMG_20220413_09235624_aug_1.jpg\n",
      "Saved augmented image: IMG_20220413_09235624_aug_2.jpg\n",
      "Saved augmented image: IMG_20220413_09235624_aug_3.jpg\n",
      "Saved augmented image: IMG_20220413_09235624_aug_4.jpg\n",
      "Saved augmented image: IMG_20220413_09235641_aug_0.jpg\n",
      "Saved augmented image: IMG_20220413_09235641_aug_1.jpg\n",
      "Saved augmented image: IMG_20220413_09235641_aug_2.jpg\n",
      "Saved augmented image: IMG_20220413_09235641_aug_3.jpg\n",
      "Saved augmented image: IMG_20220413_09235641_aug_4.jpg\n",
      "Saved augmented image: IMG_20220421_09236603_aug_0.jpg\n",
      "Saved augmented image: IMG_20220421_09236603_aug_1.jpg\n",
      "Saved augmented image: IMG_20220421_09236603_aug_2.jpg\n",
      "Saved augmented image: IMG_20220421_09236603_aug_3.jpg\n",
      "Saved augmented image: IMG_20220421_09236603_aug_4.jpg\n",
      "Saved augmented image: IMG_20220426_09237222_aug_0.jpg\n",
      "Saved augmented image: IMG_20220426_09237222_aug_1.jpg\n",
      "Saved augmented image: IMG_20220426_09237222_aug_2.jpg\n",
      "Saved augmented image: IMG_20220426_09237222_aug_3.jpg\n",
      "Saved augmented image: IMG_20220426_09237222_aug_4.jpg\n",
      "Saved augmented image: IMG_20220427_09237241_aug_0.jpg\n",
      "Saved augmented image: IMG_20220427_09237241_aug_1.jpg\n",
      "Saved augmented image: IMG_20220427_09237241_aug_2.jpg\n",
      "Saved augmented image: IMG_20220427_09237241_aug_3.jpg\n",
      "Saved augmented image: IMG_20220427_09237241_aug_4.jpg\n",
      "Saved augmented image: IMG_20220506_09238574_aug_0.jpg\n",
      "Saved augmented image: IMG_20220506_09238574_aug_1.jpg\n",
      "Saved augmented image: IMG_20220506_09238574_aug_2.jpg\n",
      "Saved augmented image: IMG_20220506_09238574_aug_3.jpg\n",
      "Saved augmented image: IMG_20220506_09238574_aug_4.jpg\n",
      "Saved augmented image: IMG_20220511_09239344_aug_0.jpg\n",
      "Saved augmented image: IMG_20220511_09239344_aug_1.jpg\n",
      "Saved augmented image: IMG_20220511_09239344_aug_2.jpg\n",
      "Saved augmented image: IMG_20220511_09239344_aug_3.jpg\n",
      "Saved augmented image: IMG_20220511_09239344_aug_4.jpg\n",
      "Saved augmented image: IMG_20220610_09245276_aug_0.jpg\n",
      "Saved augmented image: IMG_20220610_09245276_aug_1.jpg\n",
      "Saved augmented image: IMG_20220610_09245276_aug_2.jpg\n",
      "Saved augmented image: IMG_20220610_09245276_aug_3.jpg\n",
      "Saved augmented image: IMG_20220610_09245276_aug_4.jpg\n",
      "Saved augmented image: IMG_20220610_09245295_aug_0.jpg\n",
      "Saved augmented image: IMG_20220610_09245295_aug_1.jpg\n",
      "Saved augmented image: IMG_20220610_09245295_aug_2.jpg\n",
      "Saved augmented image: IMG_20220610_09245295_aug_3.jpg\n",
      "Saved augmented image: IMG_20220610_09245295_aug_4.jpg\n",
      "Saved augmented image: IMG_20220610_09245303_aug_0.jpg\n",
      "Saved augmented image: IMG_20220610_09245303_aug_1.jpg\n",
      "Saved augmented image: IMG_20220610_09245303_aug_2.jpg\n",
      "Saved augmented image: IMG_20220610_09245303_aug_3.jpg\n",
      "Saved augmented image: IMG_20220610_09245303_aug_4.jpg\n",
      "Saved augmented image: IMG_20220610_09245307_aug_0.jpg\n",
      "Saved augmented image: IMG_20220610_09245307_aug_1.jpg\n",
      "Saved augmented image: IMG_20220610_09245307_aug_2.jpg\n",
      "Saved augmented image: IMG_20220610_09245307_aug_3.jpg\n",
      "Saved augmented image: IMG_20220610_09245307_aug_4.jpg\n",
      "Saved augmented image: IMG_20220615_09246267_aug_0.jpg\n",
      "Saved augmented image: IMG_20220615_09246267_aug_1.jpg\n",
      "Saved augmented image: IMG_20220615_09246267_aug_2.jpg\n",
      "Saved augmented image: IMG_20220615_09246267_aug_3.jpg\n",
      "Saved augmented image: IMG_20220615_09246267_aug_4.jpg\n",
      "Saved augmented image: IMG_20220617_09246835_aug_0.jpg\n",
      "Saved augmented image: IMG_20220617_09246835_aug_1.jpg\n",
      "Saved augmented image: IMG_20220617_09246835_aug_2.jpg\n",
      "Saved augmented image: IMG_20220617_09246835_aug_3.jpg\n",
      "Saved augmented image: IMG_20220617_09246835_aug_4.jpg\n",
      "Saved augmented image: IMG_20220623_09247857_aug_0.jpg\n",
      "Saved augmented image: IMG_20220623_09247857_aug_1.jpg\n",
      "Saved augmented image: IMG_20220623_09247857_aug_2.jpg\n",
      "Saved augmented image: IMG_20220623_09247857_aug_3.jpg\n",
      "Saved augmented image: IMG_20220623_09247857_aug_4.jpg\n",
      "Saved new CSV with only augmented positives: C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\cleaned_labels_augmented.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, save_img\n",
    "\n",
    "# Input paths\n",
    "csv_path = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\cleaned_labels.csv\"\n",
    "image_dir = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\dataset\\content\\dataset\"\n",
    "output_dir = os.path.join(image_dir, \"augmented\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load CSV and keep only COVID-positive rows\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df['label'].isin(['0', '1'])]     # Remove bad rows like 'result not found'\n",
    "positive_df = df[df['label'] == '1']      # Correct way to select COVID-positive rows\n",
    "\n",
    "print(f\"Found {len(positive_df)} COVID-positive images to augment.\")\n",
    "\n",
    "# Image augmentor\n",
    "augmentor = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Settings\n",
    "AUG_PER_IMAGE = 5\n",
    "augmented_entries = []\n",
    "\n",
    "# Process positive images only\n",
    "for _, row in positive_df.iterrows():\n",
    "    img_path = os.path.join(image_dir, row['filename'])\n",
    "    base_name = os.path.splitext(os.path.basename(row['filename']))[0]\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"File not found: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img).reshape((1,) + img.size + (3,))  # Ensure shape is (1, height, width, 3)\n",
    "\n",
    "        for i, batch in enumerate(augmentor.flow(x, batch_size=1)):\n",
    "            aug_filename = f\"{base_name}_aug_{i}.jpg\"\n",
    "            aug_path = os.path.join(output_dir, aug_filename)\n",
    "            save_img(aug_path, batch[0])\n",
    "\n",
    "            # Store label 1 for each augmented file\n",
    "            augmented_entries.append({'filename': os.path.join('augmented', aug_filename), 'label': 1})\n",
    "            print(f\"Saved augmented image: {aug_filename}\")\n",
    "\n",
    "            if i + 1 >= AUG_PER_IMAGE:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {img_path}: {e}\")\n",
    "\n",
    "# Save new CSV with only augmented entries\n",
    "augmented_df = pd.DataFrame(augmented_entries)\n",
    "output_csv = r\"C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\cleaned_labels_augmented.csv\"\n",
    "augmented_df.to_csv(output_csv, index=False)\n",
    "print(f\"Saved new CSV with only augmented positives: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c0a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define paths\n",
    "# IMAGE_DIR = r'C:\\Users\\ajith\\OneDrive\\Desktop\\Dissertation\\dataset\\content\\dataset'\n",
    "# CSV_PATH = 'cleaned_labels.csv'  # Or 'cleaned_labels.csv'\n",
    "# df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# # Image parameters\n",
    "# img_size = (224, 224)\n",
    "# batch_size = 32\n",
    "\n",
    "# # Data augmentation for training\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=15,\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.1,\n",
    "#     shear_range=0.1,\n",
    "#     zoom_range=0.1,\n",
    "#     horizontal_flip=True,\n",
    "#     validation_split=0.2\n",
    "# )\n",
    "\n",
    "# # Only rescaling for validation\n",
    "# val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_dataframe(\n",
    "#     dataframe=df,\n",
    "#     directory=IMAGE_DIR,\n",
    "#     x_col='filename',\n",
    "#     y_col='label',\n",
    "#     target_size=img_size,\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='training',\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_generator = val_datagen.flow_from_dataframe(\n",
    "#     dataframe=df,\n",
    "#     directory=IMAGE_DIR,\n",
    "#     x_col='filename',\n",
    "#     y_col='label',\n",
    "#     target_size=img_size,\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='validation',\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# num_classes = len(train_generator.class_indices)\n",
    "# num_train_samples = train_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719b5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,     # ← mirror image enabled\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec155b72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mtrain_generator\u001b[49m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(x_batch[i])\n",
    "    label_idx = y_batch[i].argmax()\n",
    "    plt.title(f\"Label: {label_idx}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd294653",
   "metadata": {},
   "source": [
    "Phase 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95694fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08875891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "orig_dir = \"C:/Users/ajith/OneDrive/Desktop/Dissertation/dataset/content/dataset\"\n",
    "aug_dir = \"C:/Users/ajith/OneDrive/Desktop/Dissertation/dataset/content/dataset/augmented\"\n",
    "\n",
    "# Load CSVs\n",
    "df_orig = pd.read_csv(\"cleaned_labels.csv\")\n",
    "df_aug = pd.read_csv(\"cleaned_labels_augmented.csv\")\n",
    "\n",
    "# Add image paths\n",
    "df_orig['path'] = df_orig['filename'].apply(lambda x: os.path.join(orig_dir, x))\n",
    "df_aug['path'] = df_aug['filename'].apply(lambda x: os.path.join(aug_dir, x))\n",
    "\n",
    "# Merge and prepare\n",
    "df = pd.concat([df_orig, df_aug], ignore_index=True)\n",
    "df['label_str'] = df['label'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b7eb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'result not found'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare for Stratified K-Fold\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# Ensure labels are int\u001b[39;00m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues                       \u001b[38;5;66;03m# dummy X, we just need indices\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\generic.py:6662\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6656\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6657\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6658\u001b[0m     ]\n\u001b[0;32m   6660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6661\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6662\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6663\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:784\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    782\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 784\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    788\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'result not found'"
     ]
    }
   ],
   "source": [
    "# Prepare for Stratified K-Fold\n",
    "df['label'] = df['label'].astype(int)     # Ensure labels are int\n",
    "X = df.index.values                       # dummy X, we just need indices\n",
    "y = df['label'].values\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_df = df.iloc[train_idx].copy()\n",
    "    val_df = df.iloc[val_idx].copy()\n",
    "\n",
    "    # Class weight calculation\n",
    "    class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                         classes=np.unique(train_df['label']),\n",
    "                                         y=train_df['label'])\n",
    "    class_weights = dict(zip(np.unique(train_df['label']), class_weights))\n",
    "\n",
    "    # Generators\n",
    "    train_gen = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label_str',\n",
    "                                            target_size=(224, 224), class_mode='binary', batch_size=16)\n",
    "    val_gen = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label_str',\n",
    "                                          target_size=(224, 224), class_mode='binary', batch_size=16, shuffle=False)\n",
    "\n",
    "    # Model architecture\n",
    "    base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers[:-40]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss=focal_loss(), metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=20,\n",
    "                        class_weight=class_weights, verbose=1)\n",
    "\n",
    "    histories.append(history)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred_prob = model.predict(val_gen)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    y_true = val_df['label'].values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}, MCC: {mcc:.4f}\")\n",
    "    scores.append({'fold': fold+1, 'accuracy': acc, 'f1': f1, 'auc': auc, 'mcc': mcc})\n",
    "\n",
    "# Summary\n",
    "score_df = pd.DataFrame(scores)\n",
    "print(\"\\nFinal Scores per Fold:\")\n",
    "print(score_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7264d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Fold 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_str\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      8\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m \u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel_str\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m val_gen \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(val_df, x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m, y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_str\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m                                       target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# MobileNetV3 setup\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1807\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   1801\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1802\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates is deprecated, you can drop duplicates \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1803\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby using the pandas.DataFrame.drop_duplicates method.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1804\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m   1805\u001b[0m     )\n\u001b[1;32m-> 1807\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_filenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1829\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:968\u001b[0m, in \u001b[0;36mDataFrameIterator.__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# check that inputs match the required class_mode\u001b[39;00m\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    970\u001b[0m     validate_filenames\n\u001b[0;32m    971\u001b[0m ):  \u001b[38;5;66;03m# check which image files are valid and keep them\u001b[39;00m\n\u001b[0;32m    972\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_valid_filepaths(df, x_col)\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1029\u001b[0m, in \u001b[0;36mDataFrameIterator._check_params\u001b[1;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf class_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, y_col must be a list. Received \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1025\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mode, \u001b[38;5;28mtype\u001b[39m(y_col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m   1026\u001b[0m         )\n\u001b[0;32m   1027\u001b[0m     )\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;66;03m# check that filenames/filepaths column values are all strings\u001b[39;00m\n\u001b[1;32m-> 1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m))):\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll values in column x_col=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m     )\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m# check labels are string if class_mode is binary or sparse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'path'"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), 1):\n",
    "    print(f\"\\n Fold {fold}\")\n",
    "    train_df = df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    val_df = df.iloc[val_idx].copy().reset_index(drop=True)\n",
    "    train_df['label_str'] = train_df['label'].astype(str)\n",
    "    val_df['label_str'] = val_df['label'].astype(str)\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_gen = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label_str',\n",
    "                                            target_size=(224, 224), class_mode='binary', batch_size=16)\n",
    "    val_gen = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label_str',\n",
    "                                          target_size=(224, 224), class_mode='binary', batch_size=16)\n",
    "\n",
    "    # MobileNetV3 setup\n",
    "    base_model = MobileNetV3Small(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-40]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "                  metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(patience=2)\n",
    "    ]\n",
    "\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=20, callbacks=callbacks, verbose=1)\n",
    "\n",
    "    # Evaluate\n",
    "    val_gen.reset()\n",
    "    preds = model.predict(val_gen).ravel()\n",
    "    true_labels = val_gen.classes\n",
    "\n",
    "    # Threshold sweep\n",
    "    best_mcc, best_thresh = -1, 0.5\n",
    "    for t in np.arange(0.2, 0.81, 0.1):\n",
    "        pred_labels = (preds > t).astype(int)\n",
    "        mcc = matthews_corrcoef(true_labels, pred_labels)\n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_thresh = t\n",
    "\n",
    "    final_preds = (preds > best_thresh).astype(int)\n",
    "    cm = confusion_matrix(true_labels, final_preds)\n",
    "    acc = accuracy_score(true_labels, final_preds)\n",
    "    f1 = f1_score(true_labels, final_preds, zero_division=0)\n",
    "    auc = roc_auc_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, final_preds, zero_division=0)\n",
    "    recall = recall_score(true_labels, final_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Best threshold = {best_thresh:.2f} with MCC = {best_mcc:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Classification Report:\\n\", classification_report(true_labels, final_preds))\n",
    "\n",
    "    # Histogram\n",
    "    plt.hist(preds, bins=20)\n",
    "    plt.title(f\"Fold {fold} - Predicted Probabilities\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'threshold': best_thresh,\n",
    "        'mcc': best_mcc,\n",
    "        'auc': auc,\n",
    "        'f1': f1,\n",
    "        'acc': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n K-Fold Summary:\")\n",
    "for r in results:\n",
    "    print(f\"Fold {r['fold']} ➤ ACC: {r['acc']:.4f}, MCC: {r['mcc']:.4f}, AUC: {r['auc']:.4f}, F1: {r['f1']:.4f}, PREC: {r['precision']:.4f}, RECALL: {r['recall']:.4f}, THRESH: {r['threshold']:.2f}\")\n",
    "\n",
    "avg = {k: np.mean([r[k] for r in results]) for k in ['acc', 'mcc', 'auc', 'f1', 'precision', 'recall']}\n",
    "print(\"\\n Averages Across Folds:\")\n",
    "for k, v in avg.items():\n",
    "    print(f\"{k.upper()}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9/9 [==============================] - 5s 180ms/step - loss: 0.7092 - accuracy: 0.3538 - auc: 0.4822 - val_loss: 0.7490 - val_accuracy: 0.1765 - val_auc: 0.5000\n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.7500 - accuracy: 0.5231 - auc: 0.3617 - val_loss: 0.6190 - val_accuracy: 0.8235 - val_auc: 0.4286\n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.7225 - accuracy: 0.5846 - auc: 0.4016 - val_loss: 0.6404 - val_accuracy: 0.8235 - val_auc: 0.4286\n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.7244 - accuracy: 0.6000 - auc: 0.4038 - val_loss: 0.6641 - val_accuracy: 0.8235 - val_auc: 0.3929\n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.7229 - accuracy: 0.6154 - auc: 0.4216 - val_loss: 0.6909 - val_accuracy: 0.8235 - val_auc: 0.5000\n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.6875 - accuracy: 0.5231 - auc: 0.5732 - val_loss: 0.7210 - val_accuracy: 0.1765 - val_auc: 0.5000\n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 0.7299 - accuracy: 0.3538 - auc: 0.3979 - val_loss: 0.7227 - val_accuracy: 0.1765 - val_auc: 0.5595\n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.7266 - accuracy: 0.2769 - auc: 0.3846 - val_loss: 0.7024 - val_accuracy: 0.1765 - val_auc: 0.4286\n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.6769 - accuracy: 0.5538 - auc: 0.6065 - val_loss: 0.6634 - val_accuracy: 0.8235 - val_auc: 0.5000\n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.6949 - accuracy: 0.6769 - auc: 0.5318 - val_loss: 0.6537 - val_accuracy: 0.8235 - val_auc: 0.5000\n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.6882 - accuracy: 0.5692 - auc: 0.5732 - val_loss: 0.6840 - val_accuracy: 0.8235 - val_auc: 0.6429\n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.6760 - accuracy: 0.6000 - auc: 0.6036 - val_loss: 0.6835 - val_accuracy: 0.8235 - val_auc: 0.5595\n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.7120 - accuracy: 0.4000 - auc: 0.3987 - val_loss: 0.7105 - val_accuracy: 0.1765 - val_auc: 0.5357\n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.7281 - accuracy: 0.4000 - auc: 0.3757 - val_loss: 0.6868 - val_accuracy: 0.8235 - val_auc: 0.5000\n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.6806 - accuracy: 0.6615 - auc: 0.5555 - val_loss: 0.6480 - val_accuracy: 0.8235 - val_auc: 0.5000\n",
      "Epoch 16/20\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.6829 - accuracy: 0.6615 - auc: 0.5658 - val_loss: 0.6375 - val_accuracy: 0.8235 - val_auc: 0.5000\n",
      "Epoch 17/20\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.6940 - accuracy: 0.6769 - auc: 0.5178 - val_loss: 0.6567 - val_accuracy: 0.8235 - val_auc: 0.5000\n",
      "Epoch 18/20\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.7168 - accuracy: 0.6000 - auc: 0.4297 - val_loss: 0.6636 - val_accuracy: 0.8235 - val_auc: 0.4643\n",
      "Epoch 19/20\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 0.7096 - accuracy: 0.6154 - auc: 0.4320 - val_loss: 0.6351 - val_accuracy: 0.8235 - val_auc: 0.4286\n",
      "Epoch 20/20\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.7205 - accuracy: 0.6154 - auc: 0.4149 - val_loss: 0.6834 - val_accuracy: 0.8235 - val_auc: 0.5238\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 16s 345ms/step - loss: 0.7149 - accuracy: 0.3385 - auc: 0.4741 - val_loss: 0.7015 - val_accuracy: 0.1765 - val_auc: 0.4286\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 2s 236ms/step - loss: 0.7161 - accuracy: 0.3231 - auc: 0.4556 - val_loss: 0.7176 - val_accuracy: 0.1765 - val_auc: 0.2857\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 2s 231ms/step - loss: 0.7150 - accuracy: 0.3231 - auc: 0.5089 - val_loss: 0.7582 - val_accuracy: 0.1765 - val_auc: 0.5714\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 0.7008 - accuracy: 0.3538 - auc: 0.5873 - val_loss: 0.8628 - val_accuracy: 0.1765 - val_auc: 0.3929\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.7326 - accuracy: 0.3538 - auc: 0.4275 - val_loss: 0.8701 - val_accuracy: 0.1765 - val_auc: 0.4048\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 0.6876 - accuracy: 0.2769 - auc: 0.5651 - val_loss: 0.8560 - val_accuracy: 0.1765 - val_auc: 0.6667\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 0.6496 - accuracy: 0.4462 - auc: 0.7249 - val_loss: 0.7861 - val_accuracy: 0.1765 - val_auc: 0.7500\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 0.6329 - accuracy: 0.4769 - auc: 0.7811 - val_loss: 0.7669 - val_accuracy: 0.1765 - val_auc: 0.5952\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 2s 224ms/step - loss: 0.6433 - accuracy: 0.5385 - auc: 0.7589 - val_loss: 0.7046 - val_accuracy: 0.4118 - val_auc: 0.7976\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 0.6218 - accuracy: 0.5692 - auc: 0.8395 - val_loss: 0.6938 - val_accuracy: 0.5294 - val_auc: 0.5714\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train with class weights\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Step 9: Fine-tune (optional)\n",
    "base_model.trainable = True\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6fb2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 46ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Evaluate\n",
    "val_gen.reset()\n",
    "y_true = val_gen.classes\n",
    "y_probs = model.predict(val_gen).ravel()\n",
    "y_pred = (y_probs > 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "report = classification_report(y_true, y_pred, output_dict=True, target_names=[\"Negative\", \"Positive\"])\n",
    "cm = confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = report[\"accuracy\"]\n",
    "precision = report[\"Positive\"][\"precision\"]\n",
    "recall = report[\"Positive\"][\"recall\"]\n",
    "specificity = report[\"Negative\"][\"recall\"]\n",
    "f1 = report[\"Positive\"][\"f1-score\"]\n",
    "auc = roc_auc_score(y_true, y_probs)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Neg\", \"Pos\"], yticklabels=[\"Neg\", \"Pos\"])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7ea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Validation Metrics:\n",
      "Accuracy:     0.5294\n",
      "Precision:    0.2222\n",
      "Recall:       0.6667 (Sensitivity)\n",
      "Specificity:  0.5000\n",
      "F1 Score:     0.3333\n",
      "AUC:          0.5714\n",
      "MCC:          0.1273\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Validation Metrics:\")\n",
    "print(f\"Accuracy:     {accuracy:.4f}\")\n",
    "print(f\"Precision:    {precision:.4f}\")\n",
    "print(f\"Recall:       {recall:.4f} (Sensitivity)\")\n",
    "print(f\"Specificity:  {specificity:.4f}\")\n",
    "print(f\"F1 Score:     {f1:.4f}\")\n",
    "print(f\"AUC:          {auc:.4f}\")\n",
    "print(f\"MCC:          {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707d2f4",
   "metadata": {},
   "source": [
    "Phase 3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f055d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c213bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "IMG_ROOT = r'C:/Users/ajith/OneDrive/Desktop/Dissertation/dataset/content/dataset'\n",
    "NORMAL_IMG_DIR = IMG_ROOT\n",
    "AUGMENTED_IMG_DIR = os.path.join(IMG_ROOT, \"augmented\")\n",
    "\n",
    "# Load CSVs\n",
    "normal_df = pd.read_csv(r'C:/Users/ajith/OneDrive/Desktop/Dissertation/cleaned_labels.csv')\n",
    "aug_df = pd.read_csv(r'C:/Users/ajith/OneDrive/Desktop/Dissertation/cleaned_labels_augmented.csv')\n",
    "normal_df = normal_df[normal_df['label'].isin(['0', '1'])].copy()\n",
    "normal_df['label'] = normal_df['label'].astype(str).str.strip().astype(int)\n",
    "aug_df['label'] = aug_df['label'].astype(int)\n",
    "aug_df = aug_df.sample(n=min(32, len(aug_df)), random_state=42)\n",
    "df = pd.concat([normal_df, aug_df], ignore_index=True)\n",
    "\n",
    "def resolve_path(filename):\n",
    "    parts = filename.replace('\\\\', '/').split('/')\n",
    "    if parts[0] == 'augmented':\n",
    "        return os.path.join(AUGMENTED_IMG_DIR, *parts[1:])\n",
    "    return os.path.join(NORMAL_IMG_DIR, *parts)\n",
    "\n",
    "df['path'] = df['filename'].apply(resolve_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd11061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "df_0 = df[df['label'] == 0]\n",
    "df_1 = df[df['label'] == 1]\n",
    "train_0, val_0 = train_test_split(df_0, test_size=0.2, random_state=42)\n",
    "train_1, val_1 = train_test_split(df_1, test_size=0.2, random_state=42)\n",
    "train_df = pd.concat([train_0, train_1]).sample(frac=1).reset_index(drop=True)\n",
    "val_df = pd.concat([val_0, val_1]).sample(frac=1).reset_index(drop=True)\n",
    "train_df['label_str'] = train_df['label'].astype(str)\n",
    "val_df['label_str'] = val_df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedfb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90 validated image filenames belonging to 2 classes.\n",
      "Found 24 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=train_df['label'])\n",
    "class_weights = dict(zip([0, 1], class_weights))\n",
    "\n",
    "# Generators\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_gen = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label_str', target_size=(224, 224), class_mode='binary', batch_size=16)\n",
    "val_gen = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label_str', target_size=(224, 224), class_mode='binary', batch_size=16)\n",
    "\n",
    "# Build model with deeper fine-tuning\n",
    "base_model = MobileNetV3Small(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab99a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 [==============================] - 3s 170ms/step - loss: 0.6960 - accuracy: 0.4222 - auc: 0.4663 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.5833 - val_auc: 0.7286 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6941 - accuracy: 0.4222 - auc: 0.4476 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6830 - val_accuracy: 0.5833 - val_auc: 0.6429 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6938 - accuracy: 0.4222 - auc: 0.4818 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6824 - val_accuracy: 0.5833 - val_auc: 0.7857 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6930 - accuracy: 0.5000 - auc: 0.5076 - precision: 0.4545 - recall: 0.9211 - val_loss: 0.6822 - val_accuracy: 0.5833 - val_auc: 0.6071 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6934 - accuracy: 0.4667 - auc: 0.5071 - precision: 0.3214 - recall: 0.2368 - val_loss: 0.6814 - val_accuracy: 0.5833 - val_auc: 0.6429 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6934 - accuracy: 0.5778 - auc: 0.4818 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6812 - val_accuracy: 0.5833 - val_auc: 0.5214 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6951 - accuracy: 0.5778 - auc: 0.3727 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6810 - val_accuracy: 0.5833 - val_auc: 0.7143 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6937 - accuracy: 0.5778 - auc: 0.5202 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6805 - val_accuracy: 0.5833 - val_auc: 0.8429 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6939 - accuracy: 0.5778 - auc: 0.5106 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6801 - val_accuracy: 0.5833 - val_auc: 0.4929 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6934 - accuracy: 0.5778 - auc: 0.4846 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6796 - val_accuracy: 0.5833 - val_auc: 0.5929 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6945 - accuracy: 0.5778 - auc: 0.4157 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6789 - val_accuracy: 0.5833 - val_auc: 0.4286 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6932 - accuracy: 0.4000 - auc: 0.5187 - precision: 0.4000 - recall: 0.8421 - val_loss: 0.6786 - val_accuracy: 0.5833 - val_auc: 0.4786 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6935 - accuracy: 0.4111 - auc: 0.4853 - precision: 0.4138 - recall: 0.9474 - val_loss: 0.6785 - val_accuracy: 0.5833 - val_auc: 0.5714 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6937 - accuracy: 0.4222 - auc: 0.4517 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6784 - val_accuracy: 0.5833 - val_auc: 0.4643 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6935 - accuracy: 0.4222 - auc: 0.4605 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6784 - val_accuracy: 0.5833 - val_auc: 0.4643 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6934 - accuracy: 0.4222 - auc: 0.4790 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6784 - val_accuracy: 0.5833 - val_auc: 0.7714 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6933 - accuracy: 0.4222 - auc: 0.4747 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6785 - val_accuracy: 0.5833 - val_auc: 0.4643 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6932 - accuracy: 0.4222 - auc: 0.5051 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6786 - val_accuracy: 0.5833 - val_auc: 0.7357 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-05\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6931 - accuracy: 0.4222 - auc: 0.5101 - precision: 0.4222 - recall: 1.0000 - val_loss: 0.6788 - val_accuracy: 0.5833 - val_auc: 0.4643 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e45e40bd60>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "callbacks = [EarlyStopping(patience=5, restore_best_weights=True), ReduceLROnPlateau(patience=2)]\n",
    "\n",
    "# Train normally\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=20, class_weight=class_weights, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78330d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "val_gen.reset()\n",
    "preds = model.predict(val_gen)\n",
    "pred_labels = (preds > 0.5).astype(int)\n",
    "true_labels = val_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Predicted class counts: {0: 24}\n",
      "Threshold 0.3 ➤ MCC = 0.0000\n",
      "Threshold 0.4 ➤ MCC = 0.0000\n",
      "Threshold 0.5 ➤ MCC = 0.0000\n",
      "Threshold 0.6 ➤ MCC = 0.0000\n",
      "Threshold 0.7 ➤ MCC = 0.0000\n",
      "Found 10 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 16ms/step - loss: 0.6955 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6953 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6953 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000          \n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6951 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000          \n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6952 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000          \n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6948 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000          \n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6945 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6949 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6947 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6942 - accuracy: 0.4000 - auc: 0.5000 - precision: 0.4000 - recall: 1.0000\n",
      "🔍 Sample prediction probs: [0.41265482 0.41469893 0.4147175  0.41208115 0.41512987 0.41427246\n",
      " 0.41381377 0.41374892 0.41528395 0.4157284 ]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(pred_labels, return_counts=True)\n",
    "print(\"🔍 Predicted class counts:\", dict(zip(unique, counts)))\n",
    "\n",
    "# MCC threshold sweep\n",
    "for t in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    swept = (preds > t).astype(int)\n",
    "    print(f\"Threshold {t:.1f} ➤ MCC = {matthews_corrcoef(true_labels, swept):.4f}\")\n",
    "\n",
    "# Sanity check overfit on 10 samples\n",
    "tiny_df = train_df.sample(10, random_state=42)\n",
    "tiny_df['label_str'] = tiny_df['label'].astype(str)\n",
    "tiny_gen = datagen.flow_from_dataframe(tiny_df, x_col='path', y_col='label_str',\n",
    "                                       target_size=(224, 224), class_mode='binary', batch_size=2)\n",
    "model.fit(tiny_gen, epochs=10)\n",
    "\n",
    "# Show sample prediction values\n",
    "print(\"Sample prediction probs:\", preds[:10].ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d97090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.3 ➤ MCC = 0.0000\n",
      "Threshold 0.4 ➤ MCC = 0.0000\n",
      "Threshold 0.5 ➤ MCC = 0.0000\n",
      "Threshold 0.6 ➤ MCC = 0.0000\n",
      "Threshold 0.7 ➤ MCC = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Threshold sweep\n",
    "for t in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    swept_preds = (preds > t).astype(int)\n",
    "    mcc = matthews_corrcoef(true_labels, swept_preds)\n",
    "    print(f\"Threshold {t:.1f} ➤ MCC = {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d1ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.42      1.00      0.59        10\n",
      "\n",
      "    accuracy                           0.42        24\n",
      "   macro avg       0.21      0.50      0.29        24\n",
      "weighted avg       0.17      0.42      0.25        24\n",
      "\n",
      "✔️ Metrics for LR=1e-05: ACC=0.4167, MCC=0.0000, AUC=0.4429, SEN=1.0000, SPEC=0.0000, F1=0.5882, PREC=0.4167\n",
      "\n",
      "📊 Summary Across Learning Rates\n",
      "LR=1e-05 | ACC=0.4167 | MCC=0.0000 | AUC=0.4429 | SEN=1.0000 | SPEC=0.0000 | F1=0.5882 | PREC=0.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\ajith\\miniconda3\\envs\\tongue_cnn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "results[lr] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"mcc\": mcc,\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision\n",
    "    }\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "print(f\"✔️ Metrics for LR={lr}: ACC={acc:.4f}, MCC={mcc:.4f}, AUC={auc:.4f}, SEN={sensitivity:.4f}, SPEC={specificity:.4f}, F1={f1:.4f}, PREC={precision:.4f}\")\n",
    "\n",
    "# Summary of all learning rates\n",
    "print(\"\\n📊 Summary Across Learning Rates\")\n",
    "for lr, metric in results.items():\n",
    "    print(f\"LR={lr:.0e} | ACC={metric['accuracy']:.4f} | MCC={metric['mcc']:.4f} | AUC={metric['auc']:.4f} | SEN={metric['sensitivity']:.4f} | SPEC={metric['specificity']:.4f} | F1={metric['f1']:.4f} | PREC={metric['precision']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b3bd7",
   "metadata": {},
   "source": [
    "Phase 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9078ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input, GlobalAveragePooling2D, Dropout, Dense\n\u001b[0;32m      5\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241m.\u001b[39mclass_indices)\n\u001b[0;32m      8\u001b[0m base_model \u001b[38;5;241m=\u001b[39m ConvNeXtTiny(\n\u001b[0;32m      9\u001b[0m     include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39minput_shape,\n\u001b[0;32m     11\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Freeze base\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "base_model = ConvNeXtTiny(\n",
    "    include_top=False,\n",
    "    input_shape=input_shape,\n",
    "    weights=\"imagenet\",\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "# Freeze base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tongue_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
